{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import defaultdict, namedtuple\n",
    "import fileinput\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sents(lines):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        lines (Iterable[str]): the lines\n",
    "\n",
    "    Yields:\n",
    "        List[str]: the lines delimited by an empty line\n",
    "    \"\"\"\n",
    "    sent = []\n",
    "    stripped_lines = (line.strip() for line in lines)\n",
    "    for line in stripped_lines:\n",
    "        if line == '':\n",
    "            yield sent\n",
    "            sent = []\n",
    "        else:\n",
    "            sent.append(line)\n",
    "    yield sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Token = namedtuple('Token', 'sent_id word_id word bio tag')\n",
    "wnut_bio = ('B', 'I', 'O')\n",
    "wnut_tags = ('corporation', 'creative-work', 'group', 'location', 'person', 'product')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_tok(word, bio_tag, sent_id=-1, word_id=-1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        word (str): the surface form of the word\n",
    "        bio_tag (str): the tag with BIO annotation\n",
    "        sent_id (int): the sentence ID\n",
    "        word_id (int): the word ID\n",
    "\n",
    "    Returns:\n",
    "        Token\n",
    "\n",
    "    Raises:\n",
    "        ValueError\n",
    "    \"\"\"\n",
    "    if bio_tag == 'O':\n",
    "        bio, tag = 'O', 'O'\n",
    "    else:\n",
    "        bio, tag = bio_tag.split('-', 1)\n",
    "        if bio not in wnut_bio or tag not in wnut_tags:\n",
    "            raise ValueError('Invalid tag: %s %s %d %d' % (word, bio_tag, sent_id, word_id))\n",
    "    return Token(sent_id, word_id, word, bio, tag)\n",
    "\n",
    "\n",
    "def token_to_conll(tok):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        tok (Token): \n",
    "\n",
    "    Returns:\n",
    "        str:\n",
    "    \"\"\"\n",
    "    return '%s\\t%s' % (tok.word, tok.tag if tok.tag == 'O' else '%s-%s' % (tok.bio, tok.tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def line_to_toks(line, sent_id=-1, word_id=-1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        line (str): the input line\n",
    "        sent_id (int): the current sentence ID\n",
    "        word_id (int): the current word ID\n",
    "\n",
    "    Returns:\n",
    "        Dictionary[str,Token]: the gold and guess tokens stored in a dict with keys for gold and guess\n",
    "\n",
    "    Raises:\n",
    "        ValueError\n",
    "    \"\"\"\n",
    "    def make_lbl(i):\n",
    "        return 'gold' if i == 0 else 'sys_%d' % i\n",
    "\n",
    "    try:\n",
    "        fields = line.split('\\t')\n",
    "        word = fields[0]\n",
    "        return {make_lbl(i): make_tok(word, bio_tag, sent_id, word_id)\n",
    "                for i, bio_tag in enumerate(fields[1:])}\n",
    "    except ValueError:\n",
    "        raise ValueError('Invalid line: %s %d %d' % (line, sent_id, word_id))\n",
    "\n",
    "\n",
    "def sent_to_toks(sent, sent_id=-1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sent (Iterator[str]): the lines that comprise a sentence\n",
    "        sent_id (int): the sentence ID\n",
    "\n",
    "    Returns:\n",
    "        Dictionary[str,List[Token]]: the gold and guess tokens for each word in the sentence,\n",
    "        stored in a dict with keys for gold and guess\n",
    "    \"\"\"\n",
    "    toks = defaultdict(list)\n",
    "    for word_id, line in enumerate(sent):\n",
    "        for src, tok in line_to_toks(line, sent_id, word_id).items():\n",
    "            toks[src].append(tok)\n",
    "    return toks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Entity = namedtuple('Entity', 'words sent_id word_id_start word_id_stop tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entity_to_tokens(entity):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        entity (Entity): \n",
    "\n",
    "    Returns:\n",
    "        List[Token]: \n",
    "    \"\"\"\n",
    "    def get_bio(_i):\n",
    "        if entity.tag == 'O':\n",
    "            return 'O'\n",
    "        elif _i == 0:\n",
    "            return 'B'\n",
    "        else:\n",
    "            return 'I'\n",
    "\n",
    "    return [Token(entity.sent_id, entity.word_id_start + i, word, get_bio(i), entity.tag)\n",
    "            for i, word in enumerate(entity.words)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entity_to_conll(entity):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        entity (Entity): \n",
    "\n",
    "    Returns:\n",
    "        List[str]: a conll-formatted token tag\n",
    "    \"\"\"\n",
    "    return [token_to_conll(tok) for tok in entity_to_tokens(entity)]\n",
    "\n",
    "\n",
    "def get_phrases(entities):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        entities (Iterable[Entity]): \n",
    "\n",
    "    Returns:\n",
    "        Set[Tuple[str]]\n",
    "    \"\"\"\n",
    "    return {entity.words for entity in entities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_phrases_and_tags(entities):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        entities (Iterable[Entity]): \n",
    "\n",
    "    Returns:\n",
    "        Set[Tuple[Tuple[str],str]]:\n",
    "    \"\"\"\n",
    "    return {(entity.words, entity.tag) for entity in entities}\n",
    "\n",
    "\n",
    "def toks_to_entities(toks):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        toks (Iterable[Token]): the tokens in a sentence\n",
    "\n",
    "    Returns:\n",
    "        Iterable[Entity]: the corresponding entities in a sentence\n",
    "\n",
    "    Raises:\n",
    "        ValueError\n",
    "    \"\"\"\n",
    "    def make_entity(tok):\n",
    "        return Entity((tok.word, ), tok.sent_id, tok.word_id, tok.word_id+1, tok.tag)\n",
    "\n",
    "    def extend_entity(entity, tok):\n",
    "        return Entity(entity.words + (tok.word, ), entity.sent_id, entity.word_id_start, tok.word_id+1, entity.tag)\n",
    "\n",
    "    def reducer(_entities, tok):\n",
    "        last = _entities.pop()\n",
    "        if tok.bio == 'I' and tok.tag == last.tag:\n",
    "            entity = extend_entity(last, tok)\n",
    "            _entities.append(entity)\n",
    "        elif tok.bio == 'B' or (tok.bio == 'O' and tok.tag == 'O'):\n",
    "            entity = make_entity(tok)\n",
    "            _entities.extend([last, entity])\n",
    "        # invalid token sequence tag1 => I-tag2: interpret as tag1 => B-tag2\n",
    "        elif tok.bio == 'I' and tok.tag != last.tag:\n",
    "            print('Invalid tag sequence: %s => %s' % (last, tok), file=sys.stderr)\n",
    "            entity = make_entity(tok)\n",
    "            _entities.extend([last, entity])\n",
    "        else:\n",
    "            raise ValueError('Invalid tag sequence: %s %s' % (last, tok))\n",
    "        return _entities\n",
    "\n",
    "    return reduce(reducer, toks[1:], [make_entity(toks[0]), ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def non_other(entity):\n",
    "    # type: (Entity) -> bool\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        entity (Entity): \n",
    "\n",
    "    Returns:\n",
    "        bool\n",
    "    \"\"\"\n",
    "    return entity.tag != 'O'\n",
    "\n",
    "\n",
    "def filter_entities(entities, p):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        entities (Iterable[Entity]): the entities in a sentence\n",
    "        p (Call[[Entity],bool): the predicate\n",
    "\n",
    "    Returns:\n",
    "        List(Entity): the entities filtered by predicate p\n",
    "    \"\"\"\n",
    "    return [entity for entity in entities if p(entity)]\n",
    "\n",
    "\n",
    "def drop_other_entities(entities):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        entities (Iterable[Entity]): \n",
    "\n",
    "    Returns:\n",
    "        Iterator[Entity]\n",
    "    \"\"\"\n",
    "    return filter_entities(entities, non_other)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def doc_to_tokses(lines):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        lines (Iterable[str]): the lines in a document\n",
    "\n",
    "    Returns:\n",
    "        Dictionary[str,List[List[Tokens]]]: a nested list of list of tokens,\n",
    "        with one list for each sentence, stored in a dict with keys for gold and guess\n",
    "    \"\"\"\n",
    "    \n",
    "    sents = get_sents(lines)\n",
    "    tokses = defaultdict(list)\n",
    "    for sent_id, sent in enumerate(sents):\n",
    "        for src, toks in sent_to_toks(sent, sent_id).items():\n",
    "            tokses[src].append(toks)\n",
    "    return tokses\n",
    "\n",
    "\n",
    "def flatten(nested):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        nested (Iterable[Iterable[T]]): a nested iterator\n",
    "\n",
    "    Returns:\n",
    "        List[T]: the iterator flattened into a list\n",
    "    \"\"\"\n",
    "    return [x for xs in nested for x in xs]\n",
    "\n",
    "\n",
    "def doc_to_toks(lines):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        lines (Iterator[str]): the lines in a document\n",
    "\n",
    "    Returns:\n",
    "        Dictionary[str,List[Tokens]]: a lists of all tokens in the document,\n",
    "        stored in a dict with keys for gold and guess\n",
    "    \"\"\"\n",
    "    return {src: flatten(nested)\n",
    "            for src, nested in doc_to_tokses(lines).items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doc_to_entitieses(lines):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        lines (Iterator[str]): the lines in a document\n",
    "\n",
    "    Returns:\n",
    "        Dictionary[str,List[List[Entity]]]: a nested list of lists of entities,\n",
    "        stored in a dict with keys for gold and guess\n",
    "\n",
    "    \"\"\"\n",
    "    entitieses = defaultdict(list)\n",
    "    for src, tokses in doc_to_tokses(lines).items():\n",
    "        entitieses[src] = [toks_to_entities(toks) for toks in tokses]\n",
    "    return entitieses\n",
    "\n",
    "\n",
    "def doc_to_entities(lines):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        lines (Iterator[str]): the lines in a document\n",
    "\n",
    "    Returns:\n",
    "        Dictionary[str,List[Entities]]: a lists of all entities in the document,\n",
    "        stored in a dict with keys for gold and guess\n",
    "    \"\"\"\n",
    "    return {src: flatten(nested)\n",
    "            for src, nested in doc_to_entitieses(lines).items()}\n",
    "\n",
    "\n",
    "def get_tags(entities):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        entities (Iterable[Entity]): the entities in a sentence\n",
    "\n",
    "    Returns:\n",
    "        Set[str]: a set of their tags, excluding 'O'\n",
    "    \"\"\"\n",
    "    return {entity.tag for entity in entities} - {'O'}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Results = namedtuple('Results', 'gold guess correct p r f')\n",
    "rem = ('corporation', 'location', 'person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tagged_entities(entities):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        entities (Dict[str,List[Entity]]): \n",
    "\n",
    "    Returns:\n",
    "        Dict[str,List[Entity]]\n",
    "    \"\"\"\n",
    "    return {src: drop_other_entities(entities)\n",
    "            for src, entities in entities.items()}\n",
    "\n",
    "\n",
    "def get_correct(gold, guess):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        gold (Iterable[T]): \n",
    "        guess (Iterable[T]): \n",
    "\n",
    "    Returns:\n",
    "        Set[T]\n",
    "    \"\"\"\n",
    "    return set(gold) & set(guess)\n",
    "\n",
    "\n",
    "def get_tp(gold, guess):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        gold (Iterable[T]): \n",
    "        guess (Iterable[T]): \n",
    "\n",
    "    Returns:\n",
    "        Set[T]\n",
    "    \"\"\"\n",
    "    return get_correct(gold, guess)\n",
    "\n",
    "\n",
    "def get_fn(gold, guess):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        gold (Iterable[T]): \n",
    "        guess (Iterable[T]): \n",
    "\n",
    "    Returns:\n",
    "        Set[T]\n",
    "    \"\"\"\n",
    "    return set(gold) - set(guess)\n",
    "\n",
    "\n",
    "def get_fp(gold, guess):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        gold (Iterable[T]): \n",
    "        guess (Iterable[T]): \n",
    "\n",
    "    Returns:\n",
    "        Set[T]\n",
    "    \"\"\"\n",
    "    return set(guess) - set(gold)\n",
    "\n",
    "\n",
    "def get_tn(tp, fp, fn, _all):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        tp (Set[T]): \n",
    "        fp (Set[T]): \n",
    "        fn (Set[T]):\n",
    "        _all (Iterable[T]):\n",
    "\n",
    "    Returns:\n",
    "        Set[T]\n",
    "    \"\"\"\n",
    "    return set(_all) - tp - fp - fn\n",
    "\n",
    "\n",
    "def get_tp_fp_fn_tn(gold, guess, _all):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        gold (Iterator[T]): \n",
    "        guess (Iterator[T]): \n",
    "        _all (Iterator[T]):\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Set[str],Set[str],Set[str],Set[str]]:\n",
    "    \"\"\"\n",
    "    tp = get_tp(gold, guess)\n",
    "    fp = get_fp(gold, guess)\n",
    "    fn = get_fn(gold, guess)\n",
    "    tn = get_tn(tp, fp, fn, _all)\n",
    "    return tp, fp, fn, tn\n",
    "\n",
    "\n",
    "def get_tp_fp_fn_tn_phrases(gold, guess, _all):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        gold: List[Entity]\n",
    "        guess: List[Entity]\n",
    "        _all: List[Entity]\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Set[str],Set[str],Set[str],Set[str]]:\n",
    "    \"\"\"\n",
    "    all_phrases = get_phrases(_all)\n",
    "    gold_phrases = get_phrases(gold)\n",
    "    guess_phrases = get_phrases(guess)\n",
    "    correct_phrases = get_phrases(get_correct(gold, guess))\n",
    "    tp = correct_phrases\n",
    "    fp = guess_phrases - tp\n",
    "    fn = gold_phrases - tp\n",
    "    tn = get_tn(tp, fp, fn, all_phrases)\n",
    "    return tp, fp, fn, tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def calc_results(gold_entities, guess_entities, surface_form=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        gold_entities (List[Entity]): the gold standard entity annotations\n",
    "        guess_entities (List[Entity]): a system's entity guesses\n",
    "        surface_form (bool): whether or not to calculate f1-scores on the entity surface forms\n",
    "\n",
    "    Returns:\n",
    "        Results: the results stored in a namedtuple\n",
    "    \"\"\"\n",
    "    # get the correct system guesses by taking the intersection of gold and guess entities,\n",
    "    # taking into account tags and document locations\n",
    "    correct_entities = get_correct(gold_entities, guess_entities)\n",
    "    if surface_form:  # count only unique surface forms when True\n",
    "        correct_entities = get_phrases_and_tags(correct_entities)\n",
    "        gold_entities = get_phrases_and_tags(gold_entities)\n",
    "        guess_entities = get_phrases_and_tags(guess_entities)\n",
    "\n",
    "    gold = len(gold_entities)\n",
    "    guess = len(guess_entities)\n",
    "    correct = len(correct_entities)\n",
    "\n",
    "    try:\n",
    "        p = correct / float(guess)\n",
    "    except ZeroDivisionError:\n",
    "        p = 0.0\n",
    "    try:\n",
    "        r = correct / float(gold)\n",
    "    except ZeroDivisionError:\n",
    "        r = 0.0\n",
    "    try:\n",
    "        f = 2.0 * p * r / (p + r)\n",
    "    except ZeroDivisionError:\n",
    "        f = 0.0\n",
    "\n",
    "    return Results(gold, guess, correct, p, r, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fmt_results(tokens, all_entities, surface_form=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        tokens (Dict[str,List[Tokens]): a dictionary of gold and guess tokens\n",
    "        all_entities (Dict[str,List[Entity]): a dictionary of gold and guess entities\n",
    "        surface_form (bool): whether or not to calculate f1-scores on the entity surface forms\n",
    "\n",
    "    Yield:\n",
    "        str: (near) W-NUT format evaluation results\n",
    "    \"\"\"\n",
    "    _sys = 'sys_1'\n",
    "    # throw out 'O' tags to get overall p/r/f\n",
    "    tagged_entities = get_tagged_entities(all_entities)\n",
    "    results = {'all': calc_results(all_entities['gold'], all_entities[_sys], surface_form=False),\n",
    "               'tagged': calc_results(tagged_entities['gold'], tagged_entities[_sys], surface_form),\n",
    "               'tokens': calc_results(tokens['gold'], tokens[_sys], surface_form=False)}\n",
    "\n",
    "    yield('processed %d tokens with %d phrases; ' %\n",
    "          (results['tokens'].gold, results['tagged'].gold))\n",
    "    yield('found: %d phrases; correct: %d.\\n' %\n",
    "          (results['tagged'].guess, results['tagged'].correct))\n",
    "\n",
    "    # get results for each entity category\n",
    "    tags = get_tags(all_entities['gold'])\n",
    "    #tags.remove('product')\n",
    "    #tags.remove('creative-work')\n",
    "    #tags.remove('group')\n",
    "    \n",
    "    tags = [v for i, v in enumerate(tags) if v in rem]\n",
    "    \n",
    "    for tag in sorted(tags):\n",
    "        entities = {src: filter_entities(entities, lambda e: e.tag == tag)\n",
    "                    for src, entities in all_entities.items()}\n",
    "        results = calc_results(entities['gold'], entities[_sys], surface_form)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training NER - WNUT17 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# Make sure you put the mitielib folder into the python search path.  There are\n",
    "# a lot of ways to do this, here we do it programmatically with the following\n",
    "# two statements:\n",
    "parent = os.path.dirname(os.path.realpath('__file__'))\n",
    "sys.path.append(parent + '/MITIE/mitielib')\n",
    "#print(sys.path)\n",
    "\n",
    "from mitie import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15733\n"
     ]
    }
   ],
   "source": [
    "### my code ###\n",
    "words = []\n",
    "tags = []\n",
    "with open('data/emerging.dev.conll') as file:\n",
    "\n",
    "        for line in file:\n",
    "            s = (line.rstrip('\\n')).split('\\t')\n",
    "            if len(s)>1:\n",
    "                words.append(s[0])\n",
    "                tags.append(s[1])\n",
    "                #print(s[1])\n",
    "#print(type(t))\n",
    "#print(type(words))\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = ner_training_instance(words[0:9999])\n",
    "s = [] \n",
    "for i in range(0,9999):\n",
    "    #print(tags[i])\n",
    "    #sample.add_entity(xrange(0,len(words[i])), tags[i])\n",
    "    #print(\"%d:%d : %s\" % (i,i+1,tags[i]))\n",
    "    sample.add_entity(xrange(i,i+1), tags[i])\n",
    "#sample.add_entity(xrange(3,5), \"person\")\n",
    "#sample.add_entity(xrange(9,10), \"org\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# And we add another training example\n",
    "#sample2 = ner_training_instance([\"The\", \"other\", \"day\", \"at\", \"work\", \"I\", \"saw\", \"Brian\", \"Smith\", \"from\", \"CMU\", \".\"])\n",
    "#sample2.add_entity(xrange(7,9), \"person\")\n",
    "#sample2.add_entity(xrange(10,11), \"org\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = ner_trainer(\"MITIE/MITIE-models/english/total_word_feature_extractor.dat\")\n",
    "trainer.add(sample)\n",
    "trainer.num_threads = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ner = trainer.train()\n",
    "ner.save_to_disk(\"new_ner_model.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"tags:\", ner.get_possible_ner_tags())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = [\"I\", \"met\", \"with\", \"John\", \"Becker\", \"at\", \"HBU\", \".\"]\n",
    "entities = ner.extract_entities(tokens)\n",
    "# Happily, it found the correct answers, \"John Becker\" and \"HBU\" in this case which we\n",
    "# print out below.\n",
    "print (\"\\nEntities found:\", entities)\n",
    "print (\"\\nNumber of entities detected:\", len(entities))\n",
    "for e in entities:\n",
    "    range = e[0]\n",
    "    tag = e[1]\n",
    "    entity_text = \" \".join(tokens[i] for i in range)\n",
    "    print (\"    \" + tag + \": \" + entity_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Trained Model and testing on WNut test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# Make sure you put the mitielib folder into the python search path.  There are\n",
    "# a lot of ways to do this, here we do it programmatically with the following\n",
    "# two statements:\n",
    "parent = os.path.dirname(os.path.realpath('__file__'))\n",
    "sys.path.append(parent + '/MITIE/mitielib')\n",
    "\n",
    "from mitie import *\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"loading MITIE's OWN NER model...\")\n",
    "ner_model = named_entity_extractor('MITIE/MITIE-models/english/ner_model.dat')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wnut - model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"loading MITIE's WNUT Trained NER model...\")\n",
    "ner_w_model = named_entity_extractor('MITIE/MITIE-models/english/new_ner_model.dat')\n",
    "\n",
    "print(\"Tags output by this NER model:\", ner_w_model.get_possible_ner_tags())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = []\n",
    "with open('data/emerging.test.conll') as file:\n",
    "        for line in file:\n",
    "            s = (line.rstrip('\\n')).split('\\t')\n",
    "            if s[0]!='':\n",
    "                tokens.append(s[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Entities MITIE NER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitie_entities = ner_model.extract_entities(tokens)\n",
    "\n",
    "#print(\"\\nEntities found:\", entities[0])\n",
    "print(\"Number of entities detected by mitie model excluding 'O' :\", len(mitie_entities))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Entities MITIE WNUT17 NER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w_entities = ner_w_model.extract_entities(tokens)\n",
    "print(\"\\nNumber of entities detected by mitienut17 model:\", len(model_w_entities))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('w_output.txt', 'w') as the_file:\n",
    "    for e in model_w_entities:\n",
    "        range = e[0]\n",
    "        tag = e[1]\n",
    "        score = e[2]\n",
    "        score_text = \"{:0.3f}\".format(score)\n",
    "        entity_text = \" \".join(tokens[i] for i in range)   \n",
    "        the_file.write(entity_text+\"\\t\"+tag+\"\\n\")\n",
    "        #print(\"   Score: \" + score_text + \": \" + tag + \": \" + entity_text)\n",
    "        #print(\"   Score: \" + score_text + \": \" + tag )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = []\n",
    "with open('mitie_output.txt', 'w') as the_file:\n",
    "    for e in mitie_entities:\n",
    "        range = e[0]\n",
    "        tag = e[1]\n",
    "        score = e[2]\n",
    "        score_text = \"{:0.3f}\".format(score)\n",
    "        entity_text = \" \".join(tokens[i] for i in range) \n",
    "        #print(len(entity_text.split()))\n",
    "        if tag == 'ORGANIZATION':\n",
    "            tag = 'corporation'\n",
    "        if(len(entity_text.split())>1):\n",
    "            #print(entity_text)\n",
    "            for i, val in enumerate(entity_text.split(), 0):\n",
    "                if(i==0):\n",
    "                    the_file.write(val+\"\\t\"+('B-'+tag)+\"\\n\")\n",
    "                else:\n",
    "                    the_file.write(val+\"\\t\"+('I-'+tag)+\"\\n\")\n",
    "        else:\n",
    "            the_file.write(entity_text+\"\\t\"+('B-'+tag)+\"\\n\")\n",
    "        d.append(entity_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toks = []\n",
    "l = [x for x in tokens if x not in d]\n",
    "for item in tokens:\n",
    "    if item in d:\n",
    "        print(item)\n",
    "        pass\n",
    "    else:\n",
    "        toks.append(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(toks),len(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Results of MITIE Trained NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/emerging.test.conll', 'r') as the_file:\n",
    "    eval_set = the_file.read().splitlines()\n",
    "print(type(eval_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('w_output.txt', 'r') as the_file:\n",
    "    w_output = the_file.read().splitlines()\n",
    "print(type(w_output))\n",
    "#for x in enumerate(eval_set):\n",
    "#        eval_set = [j + w_output[] for j in eval_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('mtaner_output', 'r') as the_file:\n",
    "        nerw_output = the_file.read().splitlines()\n",
    "with open('test.conll', 'r') as the_file:\n",
    "        test_conll = the_file.read().splitlines()\n",
    "\n",
    "\n",
    "t = [i.split('\\t')[0] for i in w_output]\n",
    "t2 = [i.split('\\t')[0] for i in nerw_output]\n",
    "#print(t)\n",
    "#s = set(t).intersection(t2)\n",
    "\n",
    "#s = list(s)\n",
    "#print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i,x in enumerate(eval_set):   \n",
    "#    eval_set = [j + w_output[i] for j in eval_set]\n",
    "#print(s)\n",
    "mta_tags_only = []\n",
    "tags_only = [i.split('\\t')[1] for i in w_output]\n",
    "mta_tags_only = [i.split('\\t')[1] for i in nerw_output]\n",
    "\n",
    "\n",
    "#for i,x in enumerate(nerw_output):\n",
    "#        mta_tags_only.append(x.split('\\t')[1])\n",
    "        #print(x.split('\\t')[1])\n",
    "    \n",
    "print(len(test_conll))\n",
    "print(len(mta_tags_only))\n",
    "print(len(tags_only))\n",
    "print(len(eval_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MITIEWNUT17 EVal set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,x in enumerate(eval_set): \n",
    "    #print(x + '\\t'+tags_only[i])\n",
    "    eval_set[i] = x + '\\t'+tags_only[i]\n",
    "print(len(eval_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MTA17 Eval set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "or i,x in enumerate(test_conll): \n",
    "    #print(x + '\\t'+mta_tags_only[i])\n",
    "    test_conll[i] = x + '\\t'+mta_tags_only[i]\n",
    "print(len(test_conll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wnut_eval(inp):\n",
    "    # get tokens and entities\n",
    "    lines = [line for line in inp]\n",
    "    #print(lines)\n",
    "    tokens = doc_to_toks(lines)\n",
    "    entities = doc_to_entities(lines)\n",
    "    # report results\n",
    "    print(\"### ENTITY F1-SCORES ###\")\n",
    "    result = fmt_results(tokens, entities, surface_form=False)\n",
    "    l = []\n",
    "    for line in result:\n",
    "        l.append(line)\n",
    "        print(line)\n",
    "        \n",
    "    labels = ['ss']\n",
    "    firstR = l[2:5]\n",
    "    \n",
    "    labels.append(l[6])\n",
    "    secondR = l[7:9]\n",
    "    \n",
    "    labels.append(l[10])\n",
    "    tR = l[11:13]\n",
    "    \n",
    "    labels.append(l[14])\n",
    "    fuR = l[15:17]\n",
    "    \n",
    "    obj,fig = mis(firstR)\n",
    "    obj2,fig2 = mis(secondR)\n",
    "    obj3,fig3 = mis(tR)\n",
    "    obj4,fig4 = mis(fuR)\n",
    "    \n",
    "    objects = [obj,obj2,obj3,obj4]\n",
    "    figures = [fig,fig2,fig3,fig4]\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    #print(labels)\n",
    "    return objects,figures,labels\n",
    "    #for line in k:\n",
    "        #print(line)\n",
    "    #for line in fmt_results(tokens, entities, surface_form=False):\n",
    "        #print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mis(input):\n",
    "    res = []\n",
    "    for i in input:\n",
    "        res.append(i.split(':'))\n",
    "    objects = [item[0] for item in res if item[0]!=' ']\n",
    "    figures = [item[1] for item in res if item[0]!=' ']\n",
    "    return objects,figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Results of MITIEWNUT17 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#wnut_eval(eval_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Results of MITIEWNUT17 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wnut_eval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0a2214616e29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mobjects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfigures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwnut_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_conll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#print(results[0][0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wnut_eval' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "objects,figures,labels = wnut_eval(test_conll)\n",
    "#print(results[0][0])\n",
    "\n",
    "\n",
    "for index, (obj, fig) in enumerate(zip(objects, figures)):\n",
    "    y_pos = np.arange(len(fig))\n",
    "    plt.bar(y_pos, fig, align='center', alpha=0.9)\n",
    "    plt.xticks(y_pos, obj)\n",
    "    plt.ylabel('ppp')\n",
    "    plt.title(labels[index])\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
