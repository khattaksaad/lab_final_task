{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import defaultdict, namedtuple\n",
    "import fileinput\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sents(lines):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        lines (Iterable[str]): the lines\n",
    "\n",
    "    Yields:\n",
    "        List[str]: the lines delimited by an empty line\n",
    "    \"\"\"\n",
    "    sent = []\n",
    "    stripped_lines = (line.strip() for line in lines)\n",
    "    for line in stripped_lines:\n",
    "        if line == '':\n",
    "            yield sent\n",
    "            sent = []\n",
    "        else:\n",
    "            sent.append(line)\n",
    "    yield sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Token = namedtuple('Token', 'sent_id word_id word bio tag')\n",
    "wnut_bio = ('B', 'I', 'O')\n",
    "wnut_tags = ('corporation', 'creative-work', 'group', 'location', 'person', 'product')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_tok(word, bio_tag, sent_id=-1, word_id=-1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        word (str): the surface form of the word\n",
    "        bio_tag (str): the tag with BIO annotation\n",
    "        sent_id (int): the sentence ID\n",
    "        word_id (int): the word ID\n",
    "\n",
    "    Returns:\n",
    "        Token\n",
    "\n",
    "    Raises:\n",
    "        ValueError\n",
    "    \"\"\"\n",
    "    if bio_tag == 'O':\n",
    "        bio, tag = 'O', 'O'\n",
    "    else:\n",
    "        bio, tag = bio_tag.split('-', 1)\n",
    "        if bio not in wnut_bio or tag not in wnut_tags:\n",
    "            raise ValueError('Invalid tag: %s %s %d %d' % (word, bio_tag, sent_id, word_id))\n",
    "    return Token(sent_id, word_id, word, bio, tag)\n",
    "\n",
    "\n",
    "def token_to_conll(tok):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        tok (Token): \n",
    "\n",
    "    Returns:\n",
    "        str:\n",
    "    \"\"\"\n",
    "    return '%s\\t%s' % (tok.word, tok.tag if tok.tag == 'O' else '%s-%s' % (tok.bio, tok.tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def line_to_toks(line, sent_id=-1, word_id=-1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        line (str): the input line\n",
    "        sent_id (int): the current sentence ID\n",
    "        word_id (int): the current word ID\n",
    "\n",
    "    Returns:\n",
    "        Dictionary[str,Token]: the gold and guess tokens stored in a dict with keys for gold and guess\n",
    "\n",
    "    Raises:\n",
    "        ValueError\n",
    "    \"\"\"\n",
    "    def make_lbl(i):\n",
    "        return 'gold' if i == 0 else 'sys_%d' % i\n",
    "\n",
    "    try:\n",
    "        fields = line.split('\\t')\n",
    "        word = fields[0]\n",
    "        return {make_lbl(i): make_tok(word, bio_tag, sent_id, word_id)\n",
    "                for i, bio_tag in enumerate(fields[1:])}\n",
    "    except ValueError:\n",
    "        raise ValueError('Invalid line: %s %d %d' % (line, sent_id, word_id))\n",
    "\n",
    "\n",
    "def sent_to_toks(sent, sent_id=-1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sent (Iterator[str]): the lines that comprise a sentence\n",
    "        sent_id (int): the sentence ID\n",
    "\n",
    "    Returns:\n",
    "        Dictionary[str,List[Token]]: the gold and guess tokens for each word in the sentence,\n",
    "        stored in a dict with keys for gold and guess\n",
    "    \"\"\"\n",
    "    toks = defaultdict(list)\n",
    "    for word_id, line in enumerate(sent):\n",
    "        for src, tok in line_to_toks(line, sent_id, word_id).items():\n",
    "            toks[src].append(tok)\n",
    "    return toks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Entity = namedtuple('Entity', 'words sent_id word_id_start word_id_stop tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entity_to_tokens(entity):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        entity (Entity): \n",
    "\n",
    "    Returns:\n",
    "        List[Token]: \n",
    "    \"\"\"\n",
    "    def get_bio(_i):\n",
    "        if entity.tag == 'O':\n",
    "            return 'O'\n",
    "        elif _i == 0:\n",
    "            return 'B'\n",
    "        else:\n",
    "            return 'I'\n",
    "\n",
    "    return [Token(entity.sent_id, entity.word_id_start + i, word, get_bio(i), entity.tag)\n",
    "            for i, word in enumerate(entity.words)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entity_to_conll(entity):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        entity (Entity): \n",
    "\n",
    "    Returns:\n",
    "        List[str]: a conll-formatted token tag\n",
    "    \"\"\"\n",
    "    return [token_to_conll(tok) for tok in entity_to_tokens(entity)]\n",
    "\n",
    "\n",
    "def get_phrases(entities):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        entities (Iterable[Entity]): \n",
    "\n",
    "    Returns:\n",
    "        Set[Tuple[str]]\n",
    "    \"\"\"\n",
    "    return {entity.words for entity in entities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_phrases_and_tags(entities):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        entities (Iterable[Entity]): \n",
    "\n",
    "    Returns:\n",
    "        Set[Tuple[Tuple[str],str]]:\n",
    "    \"\"\"\n",
    "    return {(entity.words, entity.tag) for entity in entities}\n",
    "\n",
    "\n",
    "def toks_to_entities(toks):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        toks (Iterable[Token]): the tokens in a sentence\n",
    "\n",
    "    Returns:\n",
    "        Iterable[Entity]: the corresponding entities in a sentence\n",
    "\n",
    "    Raises:\n",
    "        ValueError\n",
    "    \"\"\"\n",
    "    def make_entity(tok):\n",
    "        return Entity((tok.word, ), tok.sent_id, tok.word_id, tok.word_id+1, tok.tag)\n",
    "\n",
    "    def extend_entity(entity, tok):\n",
    "        return Entity(entity.words + (tok.word, ), entity.sent_id, entity.word_id_start, tok.word_id+1, entity.tag)\n",
    "\n",
    "    def reducer(_entities, tok):\n",
    "        last = _entities.pop()\n",
    "        if tok.bio == 'I' and tok.tag == last.tag:\n",
    "            entity = extend_entity(last, tok)\n",
    "            _entities.append(entity)\n",
    "        elif tok.bio == 'B' or (tok.bio == 'O' and tok.tag == 'O'):\n",
    "            entity = make_entity(tok)\n",
    "            _entities.extend([last, entity])\n",
    "        # invalid token sequence tag1 => I-tag2: interpret as tag1 => B-tag2\n",
    "        elif tok.bio == 'I' and tok.tag != last.tag:\n",
    "            print('Invalid tag sequence: %s => %s' % (last, tok), file=sys.stderr)\n",
    "            entity = make_entity(tok)\n",
    "            _entities.extend([last, entity])\n",
    "        else:\n",
    "            raise ValueError('Invalid tag sequence: %s %s' % (last, tok))\n",
    "        return _entities\n",
    "\n",
    "    return reduce(reducer, toks[1:], [make_entity(toks[0]), ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def non_other(entity):\n",
    "    # type: (Entity) -> bool\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        entity (Entity): \n",
    "\n",
    "    Returns:\n",
    "        bool\n",
    "    \"\"\"\n",
    "    return entity.tag != 'O'\n",
    "\n",
    "\n",
    "def filter_entities(entities, p):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        entities (Iterable[Entity]): the entities in a sentence\n",
    "        p (Call[[Entity],bool): the predicate\n",
    "\n",
    "    Returns:\n",
    "        List(Entity): the entities filtered by predicate p\n",
    "    \"\"\"\n",
    "    return [entity for entity in entities if p(entity)]\n",
    "\n",
    "\n",
    "def drop_other_entities(entities):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        entities (Iterable[Entity]): \n",
    "\n",
    "    Returns:\n",
    "        Iterator[Entity]\n",
    "    \"\"\"\n",
    "    return filter_entities(entities, non_other)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def doc_to_tokses(lines):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        lines (Iterable[str]): the lines in a document\n",
    "\n",
    "    Returns:\n",
    "        Dictionary[str,List[List[Tokens]]]: a nested list of list of tokens,\n",
    "        with one list for each sentence, stored in a dict with keys for gold and guess\n",
    "    \"\"\"\n",
    "    \n",
    "    sents = get_sents(lines)\n",
    "    tokses = defaultdict(list)\n",
    "    for sent_id, sent in enumerate(sents):\n",
    "        for src, toks in sent_to_toks(sent, sent_id).items():\n",
    "            tokses[src].append(toks)\n",
    "    return tokses\n",
    "\n",
    "\n",
    "def flatten(nested):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        nested (Iterable[Iterable[T]]): a nested iterator\n",
    "\n",
    "    Returns:\n",
    "        List[T]: the iterator flattened into a list\n",
    "    \"\"\"\n",
    "    return [x for xs in nested for x in xs]\n",
    "\n",
    "\n",
    "def doc_to_toks(lines):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        lines (Iterator[str]): the lines in a document\n",
    "\n",
    "    Returns:\n",
    "        Dictionary[str,List[Tokens]]: a lists of all tokens in the document,\n",
    "        stored in a dict with keys for gold and guess\n",
    "    \"\"\"\n",
    "    return {src: flatten(nested)\n",
    "            for src, nested in doc_to_tokses(lines).items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doc_to_entitieses(lines):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        lines (Iterator[str]): the lines in a document\n",
    "\n",
    "    Returns:\n",
    "        Dictionary[str,List[List[Entity]]]: a nested list of lists of entities,\n",
    "        stored in a dict with keys for gold and guess\n",
    "\n",
    "    \"\"\"\n",
    "    entitieses = defaultdict(list)\n",
    "    for src, tokses in doc_to_tokses(lines).items():\n",
    "        entitieses[src] = [toks_to_entities(toks) for toks in tokses]\n",
    "    return entitieses\n",
    "\n",
    "\n",
    "def doc_to_entities(lines):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        lines (Iterator[str]): the lines in a document\n",
    "\n",
    "    Returns:\n",
    "        Dictionary[str,List[Entities]]: a lists of all entities in the document,\n",
    "        stored in a dict with keys for gold and guess\n",
    "    \"\"\"\n",
    "    return {src: flatten(nested)\n",
    "            for src, nested in doc_to_entitieses(lines).items()}\n",
    "\n",
    "\n",
    "def get_tags(entities):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        entities (Iterable[Entity]): the entities in a sentence\n",
    "\n",
    "    Returns:\n",
    "        Set[str]: a set of their tags, excluding 'O'\n",
    "    \"\"\"\n",
    "    return {entity.tag for entity in entities} - {'O'}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Results = namedtuple('Results', 'gold guess correct p r f')\n",
    "rem = ('corporation', 'location', 'person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tagged_entities(entities):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        entities (Dict[str,List[Entity]]): \n",
    "\n",
    "    Returns:\n",
    "        Dict[str,List[Entity]]\n",
    "    \"\"\"\n",
    "    return {src: drop_other_entities(entities)\n",
    "            for src, entities in entities.items()}\n",
    "\n",
    "\n",
    "def get_correct(gold, guess):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        gold (Iterable[T]): \n",
    "        guess (Iterable[T]): \n",
    "\n",
    "    Returns:\n",
    "        Set[T]\n",
    "    \"\"\"\n",
    "    return set(gold) & set(guess)\n",
    "\n",
    "\n",
    "def get_tp(gold, guess):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        gold (Iterable[T]): \n",
    "        guess (Iterable[T]): \n",
    "\n",
    "    Returns:\n",
    "        Set[T]\n",
    "    \"\"\"\n",
    "    return get_correct(gold, guess)\n",
    "\n",
    "\n",
    "def get_fn(gold, guess):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        gold (Iterable[T]): \n",
    "        guess (Iterable[T]): \n",
    "\n",
    "    Returns:\n",
    "        Set[T]\n",
    "    \"\"\"\n",
    "    return set(gold) - set(guess)\n",
    "\n",
    "\n",
    "def get_fp(gold, guess):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        gold (Iterable[T]): \n",
    "        guess (Iterable[T]): \n",
    "\n",
    "    Returns:\n",
    "        Set[T]\n",
    "    \"\"\"\n",
    "    return set(guess) - set(gold)\n",
    "\n",
    "\n",
    "def get_tn(tp, fp, fn, _all):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        tp (Set[T]): \n",
    "        fp (Set[T]): \n",
    "        fn (Set[T]):\n",
    "        _all (Iterable[T]):\n",
    "\n",
    "    Returns:\n",
    "        Set[T]\n",
    "    \"\"\"\n",
    "    return set(_all) - tp - fp - fn\n",
    "\n",
    "\n",
    "def get_tp_fp_fn_tn(gold, guess, _all):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        gold (Iterator[T]): \n",
    "        guess (Iterator[T]): \n",
    "        _all (Iterator[T]):\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Set[str],Set[str],Set[str],Set[str]]:\n",
    "    \"\"\"\n",
    "    tp = get_tp(gold, guess)\n",
    "    fp = get_fp(gold, guess)\n",
    "    fn = get_fn(gold, guess)\n",
    "    tn = get_tn(tp, fp, fn, _all)\n",
    "    return tp, fp, fn, tn\n",
    "\n",
    "\n",
    "def get_tp_fp_fn_tn_phrases(gold, guess, _all):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        gold: List[Entity]\n",
    "        guess: List[Entity]\n",
    "        _all: List[Entity]\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Set[str],Set[str],Set[str],Set[str]]:\n",
    "    \"\"\"\n",
    "    all_phrases = get_phrases(_all)\n",
    "    gold_phrases = get_phrases(gold)\n",
    "    guess_phrases = get_phrases(guess)\n",
    "    correct_phrases = get_phrases(get_correct(gold, guess))\n",
    "    tp = correct_phrases\n",
    "    fp = guess_phrases - tp\n",
    "    fn = gold_phrases - tp\n",
    "    tn = get_tn(tp, fp, fn, all_phrases)\n",
    "    return tp, fp, fn, tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def calc_results(gold_entities, guess_entities, surface_form=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        gold_entities (List[Entity]): the gold standard entity annotations\n",
    "        guess_entities (List[Entity]): a system's entity guesses\n",
    "        surface_form (bool): whether or not to calculate f1-scores on the entity surface forms\n",
    "\n",
    "    Returns:\n",
    "        Results: the results stored in a namedtuple\n",
    "    \"\"\"\n",
    "    # get the correct system guesses by taking the intersection of gold and guess entities,\n",
    "    # taking into account tags and document locations\n",
    "    correct_entities = get_correct(gold_entities, guess_entities)\n",
    "    if surface_form:  # count only unique surface forms when True\n",
    "        correct_entities = get_phrases_and_tags(correct_entities)\n",
    "        gold_entities = get_phrases_and_tags(gold_entities)\n",
    "        guess_entities = get_phrases_and_tags(guess_entities)\n",
    "\n",
    "    gold = len(gold_entities)\n",
    "    guess = len(guess_entities)\n",
    "    correct = len(correct_entities)\n",
    "\n",
    "    try:\n",
    "        p = correct / float(guess)\n",
    "    except ZeroDivisionError:\n",
    "        p = 0.0\n",
    "    try:\n",
    "        r = correct / float(gold)\n",
    "    except ZeroDivisionError:\n",
    "        r = 0.0\n",
    "    try:\n",
    "        f = 2.0 * p * r / (p + r)\n",
    "    except ZeroDivisionError:\n",
    "        f = 0.0\n",
    "\n",
    "    return Results(gold, guess, correct, p, r, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fmt_results(tokens, all_entities, surface_form=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        tokens (Dict[str,List[Tokens]): a dictionary of gold and guess tokens\n",
    "        all_entities (Dict[str,List[Entity]): a dictionary of gold and guess entities\n",
    "        surface_form (bool): whether or not to calculate f1-scores on the entity surface forms\n",
    "\n",
    "    Yield:\n",
    "        str: (near) W-NUT format evaluation results\n",
    "    \"\"\"\n",
    "    _sys = 'sys_1'\n",
    "    # throw out 'O' tags to get overall p/r/f\n",
    "    tagged_entities = get_tagged_entities(all_entities)\n",
    "    results = {'all': calc_results(all_entities['gold'], all_entities[_sys], surface_form=False),\n",
    "               'tagged': calc_results(tagged_entities['gold'], tagged_entities[_sys], surface_form),\n",
    "               'tokens': calc_results(tokens['gold'], tokens[_sys], surface_form=False)}\n",
    "\n",
    "    yield('processed %d tokens with %d phrases; ' %\n",
    "          (results['tokens'].gold, results['tagged'].gold))\n",
    "    yield('found: %d phrases; correct: %d.\\n' %\n",
    "          (results['tagged'].guess, results['tagged'].correct))\n",
    "\n",
    "    if results['tokens'].gold > 0:\n",
    "        # only use token counts for accuracy\n",
    "        yield('accuracy: %6.2f; ' %\n",
    "              (100. * results['tokens'].correct / results['tokens'].gold))\n",
    "        yield('precision: %6.2f; ' % (100. * results['tagged'].p))\n",
    "        yield('recall: %6.2f; ' % (100. * results['tagged'].r))\n",
    "        yield('FB1: %6.2f\\n' % (100. * results['tagged'].f))\n",
    "\n",
    "    # get results for each entity category\n",
    "    tags = get_tags(all_entities['gold'])\n",
    "    #tags.remove('product')\n",
    "    #tags.remove('creative-work')\n",
    "    #tags.remove('group')\n",
    "    \n",
    "    tags = [v for i, v in enumerate(tags) if v in rem]\n",
    "    \n",
    "    print(tags)\n",
    "    for tag in sorted(tags):\n",
    "        entities = {src: filter_entities(entities, lambda e: e.tag == tag)\n",
    "                    for src, entities in all_entities.items()}\n",
    "        results = calc_results(entities['gold'], entities[_sys], surface_form)\n",
    "        yield('%17s: ' % tag)\n",
    "        yield('precision: %6.2f; ' % (100. * results.p))\n",
    "        yield('recall: %6.2f; ' % (100. * results.r))\n",
    "        yield('FB1: %6.2f  %d\\n' % (100. * results.f, results.correct))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training NER - WNUT17 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# Make sure you put the mitielib folder into the python search path.  There are\n",
    "# a lot of ways to do this, here we do it programmatically with the following\n",
    "# two statements:\n",
    "parent = os.path.dirname(os.path.realpath('__file__'))\n",
    "sys.path.append(parent + '/MITIE/mitielib')\n",
    "#print(sys.path)\n",
    "\n",
    "from mitie import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15733\n"
     ]
    }
   ],
   "source": [
    "### my code ###\n",
    "words = []\n",
    "tags = []\n",
    "with open('data/emerging.dev.conll') as file:\n",
    "\n",
    "        for line in file:\n",
    "            s = (line.rstrip('\\n')).split('\\t')\n",
    "            if len(s)>1:\n",
    "                words.append(s[0])\n",
    "                tags.append(s[1])\n",
    "                #print(s[1])\n",
    "#print(type(t))\n",
    "#print(type(words))\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = ner_training_instance(words[0:9999])\n",
    "s = [] \n",
    "for i in range(0,9999):\n",
    "    #print(tags[i])\n",
    "    #sample.add_entity(xrange(0,len(words[i])), tags[i])\n",
    "    #print(\"%d:%d : %s\" % (i,i+1,tags[i]))\n",
    "    sample.add_entity(xrange(i,i+1), tags[i])\n",
    "#sample.add_entity(xrange(3,5), \"person\")\n",
    "#sample.add_entity(xrange(9,10), \"org\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# And we add another training example\n",
    "#sample2 = ner_training_instance([\"The\", \"other\", \"day\", \"at\", \"work\", \"I\", \"saw\", \"Brian\", \"Smith\", \"from\", \"CMU\", \".\"])\n",
    "#sample2.add_entity(xrange(7,9), \"person\")\n",
    "#sample2.add_entity(xrange(10,11), \"org\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = ner_trainer(\"MITIE/MITIE-models/english/total_word_feature_extractor.dat\")\n",
    "trainer.add(sample)\n",
    "trainer.num_threads = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ner = trainer.train()\n",
    "ner.save_to_disk(\"new_ner_model.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"tags:\", ner.get_possible_ner_tags())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = [\"I\", \"met\", \"with\", \"John\", \"Becker\", \"at\", \"HBU\", \".\"]\n",
    "entities = ner.extract_entities(tokens)\n",
    "# Happily, it found the correct answers, \"John Becker\" and \"HBU\" in this case which we\n",
    "# print out below.\n",
    "print (\"\\nEntities found:\", entities)\n",
    "print (\"\\nNumber of entities detected:\", len(entities))\n",
    "for e in entities:\n",
    "    range = e[0]\n",
    "    tag = e[1]\n",
    "    entity_text = \" \".join(tokens[i] for i in range)\n",
    "    print (\"    \" + tag + \": \" + entity_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Trained Model and testing on WNut test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# Make sure you put the mitielib folder into the python search path.  There are\n",
    "# a lot of ways to do this, here we do it programmatically with the following\n",
    "# two statements:\n",
    "parent = os.path.dirname(os.path.realpath('__file__'))\n",
    "sys.path.append(parent + '/MITIE/mitielib')\n",
    "\n",
    "from mitie import *\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading MITIE's OWN NER model...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"loading MITIE's OWN NER model...\")\n",
    "ner_model = named_entity_extractor('MITIE/MITIE-models/english/ner_model.dat')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wnut - model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading MITIE's WNUT Trained NER model...\n",
      "Tags output by this NER model: ['O', 'B-location', 'I-location', 'B-person', 'I-person', 'B-corporation', 'I-corporation']\n"
     ]
    }
   ],
   "source": [
    "print(\"loading MITIE's WNUT Trained NER model...\")\n",
    "ner_w_model = named_entity_extractor('MITIE/MITIE-models/english/new_ner_model.dat')\n",
    "\n",
    "print(\"Tags output by this NER model:\", ner_w_model.get_possible_ner_tags())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = []\n",
    "with open('data/emerging.test.conll') as file:\n",
    "        for line in file:\n",
    "            s = (line.rstrip('\\n')).split('\\t')\n",
    "            if s[0]!='':\n",
    "                tokens.append(s[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Entities MITIE NER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entities detected by mitie model excluding 'O' : 655\n"
     ]
    }
   ],
   "source": [
    "mitie_entities = ner_model.extract_entities(tokens)\n",
    "\n",
    "#print(\"\\nEntities found:\", entities[0])\n",
    "print(\"Number of entities detected by mitie model excluding 'O' :\", len(mitie_entities))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Entities MITIE WNUT17 NER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of entities detected by mitienut17 model: 22672\n"
     ]
    }
   ],
   "source": [
    "model_w_entities = ner_w_model.extract_entities(tokens)\n",
    "print(\"\\nNumber of entities detected by mitienut17 model:\", len(model_w_entities))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('w_output.txt', 'w') as the_file:\n",
    "    for e in model_w_entities:\n",
    "        range = e[0]\n",
    "        tag = e[1]\n",
    "        score = e[2]\n",
    "        score_text = \"{:0.3f}\".format(score)\n",
    "        entity_text = \" \".join(tokens[i] for i in range)   \n",
    "        the_file.write(entity_text+\"\\t\"+tag+\"\\n\")\n",
    "        #print(\"   Score: \" + score_text + \": \" + tag + \": \" + entity_text)\n",
    "        #print(\"   Score: \" + score_text + \": \" + tag )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = []\n",
    "with open('mitie_output.txt', 'w') as the_file:\n",
    "    for e in mitie_entities:\n",
    "        range = e[0]\n",
    "        tag = e[1]\n",
    "        score = e[2]\n",
    "        score_text = \"{:0.3f}\".format(score)\n",
    "        entity_text = \" \".join(tokens[i] for i in range) \n",
    "        #print(len(entity_text.split()))\n",
    "        if tag == 'ORGANIZATION':\n",
    "            tag = 'corporation'\n",
    "        if(len(entity_text.split())>1):\n",
    "            #print(entity_text)\n",
    "            for i, val in enumerate(entity_text.split(), 0):\n",
    "                if(i==0):\n",
    "                    the_file.write(val+\"\\t\"+('B-'+tag)+\"\\n\")\n",
    "                else:\n",
    "                    the_file.write(val+\"\\t\"+('I-'+tag)+\"\\n\")\n",
    "        else:\n",
    "            the_file.write(entity_text+\"\\t\"+('B-'+tag)+\"\\n\")\n",
    "        d.append(entity_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toks = []\n",
    "l = [x for x in tokens if x not in d]\n",
    "for item in tokens:\n",
    "    if item in d:\n",
    "        print(item)\n",
    "        pass\n",
    "    else:\n",
    "        toks.append(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22672 22672\n"
     ]
    }
   ],
   "source": [
    "print(len(toks),len(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Results of MITIE Trained NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "with open('data/emerging.test.conll', 'r') as the_file:\n",
    "    eval_set = the_file.read().splitlines()\n",
    "print(type(eval_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "with open('w_output.txt', 'r') as the_file:\n",
    "    w_output = the_file.read().splitlines()\n",
    "print(type(w_output))\n",
    "#for x in enumerate(eval_set):\n",
    "#        eval_set = [j + w_output[] for j in eval_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('mtaner_output', 'r') as the_file:\n",
    "        nerw_output = the_file.read().splitlines()\n",
    "with open('test.conll', 'r') as the_file:\n",
    "        test_conll = the_file.read().splitlines()\n",
    "\n",
    "\n",
    "t = [i.split('\\t')[0] for i in w_output]\n",
    "t2 = [i.split('\\t')[0] for i in nerw_output]\n",
    "#print(t)\n",
    "#s = set(t).intersection(t2)\n",
    "\n",
    "#s = list(s)\n",
    "#print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23394\n",
      "23394\n",
      "22672\n",
      "22672\n"
     ]
    }
   ],
   "source": [
    "#for i,x in enumerate(eval_set):   \n",
    "#    eval_set = [j + w_output[i] for j in eval_set]\n",
    "#print(s)\n",
    "mta_tags_only = []\n",
    "tags_only = [i.split('\\t')[1] for i in w_output]\n",
    "mta_tags_only = [i.split('\\t')[1] for i in nerw_output]\n",
    "\n",
    "\n",
    "#for i,x in enumerate(nerw_output):\n",
    "#        mta_tags_only.append(x.split('\\t')[1])\n",
    "        #print(x.split('\\t')[1])\n",
    "    \n",
    "print(len(test_conll))\n",
    "print(len(mta_tags_only))\n",
    "print(len(tags_only))\n",
    "print(len(eval_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MITIEWNUT17 EVal set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22672\n"
     ]
    }
   ],
   "source": [
    "for i,x in enumerate(eval_set): \n",
    "    #print(x + '\\t'+tags_only[i])\n",
    "    eval_set[i] = x + '\\t'+tags_only[i]\n",
    "print(len(eval_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MTA17 Eval set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23394\n"
     ]
    }
   ],
   "source": [
    "for i,x in enumerate(test_conll): \n",
    "    #print(x + '\\t'+mta_tags_only[i])\n",
    "    test_conll[i] = x + '\\t'+mta_tags_only[i]\n",
    "print(len(test_conll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wnut_eval(inp):\n",
    "    # get tokens and entities\n",
    "    lines = [line for line in inp]\n",
    "    #print(lines)\n",
    "    tokens = doc_to_toks(lines)\n",
    "    entities = doc_to_entities(lines)\n",
    "    # report results\n",
    "    print(\"### ENTITY F1-SCORES ###\")\n",
    "    result = fmt_results(tokens, entities, surface_form=False)\n",
    "    l = []\n",
    "    for line in result:\n",
    "        line = line.replace(';','')\n",
    "        l.append(line)\n",
    "        print(line)\n",
    "        \n",
    "    labels = ['ss']\n",
    "    firstR = l[2:5]\n",
    "    \n",
    "    labels.append(l[6])\n",
    "    secondR = l[7:9]\n",
    "    \n",
    "    labels.append(l[10])\n",
    "    tR = l[11:13]\n",
    "    \n",
    "    labels.append(l[14])\n",
    "    fuR = l[15:17]\n",
    "    \n",
    "    obj,fig = mis(firstR)\n",
    "    obj2,fig2 = mis(secondR)\n",
    "    obj3,fig3 = mis(tR)\n",
    "    obj4,fig4 = mis(fuR)\n",
    "    \n",
    "    objects = [obj,obj2,obj3,obj4]\n",
    "    figures = [fig,fig2,fig3,fig4]\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    #print(fig)\n",
    "    return objects,figures,labels\n",
    "    #for line in k:\n",
    "        #print(line)\n",
    "    #for line in fmt_results(tokens, entities, surface_form=False):\n",
    "        #print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mis(input):\n",
    "    res = []\n",
    "    for i in input:\n",
    "        res.append(i.split(':'))\n",
    "    objects = [item[0] for item in res if item[0]!=' ']\n",
    "    figures = [item[1] for item in res if item[0]!=' ']\n",
    "    figures = map(float, figures)\n",
    "    return objects,figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Results of MITIEWNUT17 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid tag sequence: Entity(words=('Smith',), sent_id=0, word_id_start=871, word_id_stop=872, tag='person') => Token(sent_id=0, word_id=872, word='Tower', bio='I', tag='location')\n",
      "Invalid tag sequence: Entity(words=('Europe',), sent_id=0, word_id_start=1056, word_id_stop=1057, tag='location') => Token(sent_id=0, word_id=1057, word='Cheney', bio='I', tag='person')\n",
      "Invalid tag sequence: Entity(words=('North',), sent_id=0, word_id_start=1987, word_id_stop=1988, tag='O') => Token(sent_id=0, word_id=1988, word='Korea', bio='I', tag='location')\n",
      "Invalid tag sequence: Entity(words=('chocolate',), sent_id=0, word_id_start=6245, word_id_stop=6246, tag='O') => Token(sent_id=0, word_id=6246, word='frog', bio='I', tag='person')\n",
      "Invalid tag sequence: Entity(words=('Minas',), sent_id=0, word_id_start=7888, word_id_stop=7889, tag='O') => Token(sent_id=0, word_id=7889, word='Tirith', bio='I', tag='person')\n",
      "Invalid tag sequence: Entity(words=('kill',), sent_id=0, word_id_start=8246, word_id_stop=8247, tag='O') => Token(sent_id=0, word_id=8247, word='Voldemort', bio='I', tag='person')\n",
      "Invalid tag sequence: Entity(words=('Synopsis',), sent_id=0, word_id_start=18430, word_id_stop=18431, tag='O') => Token(sent_id=0, word_id=18431, word='Lee', bio='I', tag='person')\n",
      "Invalid tag sequence: Entity(words=(\"'\",), sent_id=0, word_id_start=4698, word_id_stop=4699, tag='O') => Token(sent_id=0, word_id=4699, word='Droids', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('between',), sent_id=0, word_id_start=5511, word_id_stop=5512, tag='O') => Token(sent_id=0, word_id=5512, word='galvanometer', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('and',), sent_id=0, word_id_start=5513, word_id_stop=5514, tag='O') => Token(sent_id=0, word_id=5514, word='galvanometer', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('over',), sent_id=0, word_id_start=5574, word_id_stop=5575, tag='O') => Token(sent_id=0, word_id=5575, word='Object', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('to',), sent_id=0, word_id_start=5643, word_id_stop=5644, tag='O') => Token(sent_id=0, word_id=5644, word='S', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('a',), sent_id=0, word_id_start=5668, word_id_stop=5669, tag='O') => Token(sent_id=0, word_id=5669, word='.', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('the',), sent_id=0, word_id_start=6244, word_id_stop=6245, tag='O') => Token(sent_id=0, word_id=6245, word='chocolate', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('ensure',), sent_id=0, word_id_start=6341, word_id_stop=6342, tag='O') => Token(sent_id=0, word_id=6342, word='certificate', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('of',), sent_id=0, word_id_start=6714, word_id_stop=6715, tag='O') => Token(sent_id=0, word_id=6715, word='U', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('?',), sent_id=0, word_id_start=6740, word_id_stop=6741, tag='O') => Token(sent_id=0, word_id=6741, word='Echo', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('the',), sent_id=0, word_id_start=7101, word_id_stop=7102, tag='O') => Token(sent_id=0, word_id=7102, word='font', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('.',), sent_id=0, word_id_start=7139, word_id_stop=7140, tag='O') => Token(sent_id=0, word_id=7140, word='12', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('.',), sent_id=0, word_id_start=7144, word_id_stop=7145, tag='O') => Token(sent_id=0, word_id=7145, word='format', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('.',), sent_id=0, word_id_start=7151, word_id_stop=7152, tag='O') => Token(sent_id=0, word_id=7152, word='12', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('need',), sent_id=0, word_id_start=7162, word_id_stop=7163, tag='O') => Token(sent_id=0, word_id=7163, word='12', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('was',), sent_id=0, word_id_start=7182, word_id_stop=7183, tag='O') => Token(sent_id=0, word_id=7183, word='12', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=(')',), sent_id=0, word_id_start=7463, word_id_stop=7464, tag='O') => Token(sent_id=0, word_id=7464, word='Control', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('other',), sent_id=0, word_id_start=7554, word_id_stop=7555, tag='O') => Token(sent_id=0, word_id=7555, word='drives', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('a',), sent_id=0, word_id_start=8331, word_id_stop=8332, tag='O') => Token(sent_id=0, word_id=8332, word='Event', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('by',), sent_id=0, word_id_start=8376, word_id_stop=8377, tag='O') => Token(sent_id=0, word_id=8377, word='robot', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('a',), sent_id=0, word_id_start=8779, word_id_stop=8780, tag='O') => Token(sent_id=0, word_id=8780, word='master', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('the',), sent_id=0, word_id_start=8786, word_id_stop=8787, tag='O') => Token(sent_id=0, word_id=8787, word='card', bio='I', tag='product')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### ENTITY F1-SCORES ###\n",
      "processed 22672 tokens with 702 phrases \n",
      "found: 306 phrases correct: 183.\n",
      "\n",
      "accuracy:  96.43 \n",
      "precision:  59.80 \n",
      "recall:  26.07 \n",
      "FB1:  36.31\n",
      "\n",
      "['person', 'location', 'corporation']\n",
      "      corporation: \n",
      "precision: 100.00 \n",
      "recall:   1.52 \n",
      "FB1:   2.99  1\n",
      "\n",
      "         location: \n",
      "precision:  66.67 \n",
      "recall:  20.00 \n",
      "FB1:  30.77  30\n",
      "\n",
      "           person: \n",
      "precision:  58.46 \n",
      "recall:  35.43 \n",
      "FB1:  44.12  152\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid tag sequence: Entity(words=(',',), sent_id=0, word_id_start=9059, word_id_stop=9060, tag='O') => Token(sent_id=0, word_id=9060, word=\"'\", bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('the',), sent_id=0, word_id_start=9315, word_id_stop=9316, tag='O') => Token(sent_id=0, word_id=9316, word='server', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('my',), sent_id=0, word_id_start=9910, word_id_stop=9911, tag='O') => Token(sent_id=0, word_id=9911, word='2', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('About',), sent_id=0, word_id_start=10110, word_id_stop=10111, tag='O') => Token(sent_id=0, word_id=10111, word='Hidden', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('the',), sent_id=0, word_id_start=10736, word_id_stop=10737, tag='O') => Token(sent_id=0, word_id=10737, word='Hallow', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('true',), sent_id=0, word_id_start=10740, word_id_stop=10741, tag='O') => Token(sent_id=0, word_id=10741, word='of', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('does',), sent_id=0, word_id_start=10998, word_id_stop=10999, tag='O') => Token(sent_id=0, word_id=10999, word='Hello', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('on',), sent_id=0, word_id_start=11004, word_id_stop=11005, tag='O') => Token(sent_id=0, word_id=11005, word='10', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('a',), sent_id=0, word_id_start=11320, word_id_stop=11321, tag='O') => Token(sent_id=0, word_id=11321, word='network', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('the',), sent_id=0, word_id_start=11336, word_id_stop=11337, tag='O') => Token(sent_id=0, word_id=11337, word='chip', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('or',), sent_id=0, word_id_start=11353, word_id_stop=11354, tag='O') => Token(sent_id=0, word_id=11354, word='card', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('bruteforce',), sent_id=0, word_id_start=12235, word_id_stop=12236, tag='O') => Token(sent_id=0, word_id=12236, word='Cracker', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('official',), sent_id=0, word_id_start=13995, word_id_stop=13996, tag='O') => Token(sent_id=0, word_id=13996, word='belt', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('technology',), sent_id=0, word_id_start=14259, word_id_stop=14260, tag='O') => Token(sent_id=0, word_id=14260, word='_', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('auction',), sent_id=0, word_id_start=14311, word_id_stop=14312, tag='O') => Token(sent_id=0, word_id=14312, word='Chevrolet', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('102520',), sent_id=0, word_id_start=14316, word_id_stop=14317, tag='O') => Token(sent_id=0, word_id=14317, word='Sport', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('little',), sent_id=0, word_id_start=14549, word_id_stop=14550, tag='O') => Token(sent_id=0, word_id=14550, word='Madness', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('affect',), sent_id=0, word_id_start=14853, word_id_stop=14854, tag='O') => Token(sent_id=0, word_id=14854, word='35', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('#',), sent_id=0, word_id_start=15178, word_id_stop=15179, tag='O') => Token(sent_id=0, word_id=15179, word='Air', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('s',), sent_id=0, word_id_start=15453, word_id_stop=15454, tag='O') => Token(sent_id=0, word_id=15454, word='sorbet', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('mud',), sent_id=0, word_id_start=17054, word_id_stop=17055, tag='O') => Token(sent_id=0, word_id=17055, word='Inspiron', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('for',), sent_id=0, word_id_start=17649, word_id_stop=17650, tag='O') => Token(sent_id=0, word_id=17650, word='OS', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('on',), sent_id=0, word_id_start=18050, word_id_stop=18051, tag='O') => Token(sent_id=0, word_id=18051, word='95', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('-',), sent_id=0, word_id_start=19261, word_id_stop=19262, tag='O') => Token(sent_id=0, word_id=19262, word='Search', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('our',), sent_id=0, word_id_start=19612, word_id_stop=19613, tag='O') => Token(sent_id=0, word_id=19613, word='Into', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('their',), sent_id=0, word_id_start=20632, word_id_stop=20633, tag='O') => Token(sent_id=0, word_id=20633, word='mixes', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('found',), sent_id=0, word_id_start=21799, word_id_stop=21800, tag='O') => Token(sent_id=0, word_id=21800, word='6', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('|',), sent_id=0, word_id_start=21802, word_id_stop=21803, tag='O') => Token(sent_id=0, word_id=21803, word='6', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('#',), sent_id=0, word_id_start=21809, word_id_stop=21810, tag='O') => Token(sent_id=0, word_id=21810, word='6', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('#',), sent_id=0, word_id_start=21812, word_id_stop=21813, tag='O') => Token(sent_id=0, word_id=21813, word='6', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('#',), sent_id=0, word_id_start=21814, word_id_stop=21815, tag='O') => Token(sent_id=0, word_id=21815, word='6', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('#',), sent_id=0, word_id_start=21817, word_id_stop=21818, tag='O') => Token(sent_id=0, word_id=21818, word='6', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('#',), sent_id=0, word_id_start=21820, word_id_stop=21821, tag='O') => Token(sent_id=0, word_id=21821, word='6', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('#',), sent_id=0, word_id_start=21823, word_id_stop=21824, tag='O') => Token(sent_id=0, word_id=21824, word='6', bio='I', tag='product')\n",
      "Invalid tag sequence: Entity(words=('(',), sent_id=0, word_id_start=21961, word_id_stop=21962, tag='O') => Token(sent_id=0, word_id=21962, word='4', bio='I', tag='product')\n"
     ]
    }
   ],
   "source": [
    "objects,figures,labels = wnut_eval(eval_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f345c24ff50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2)\n",
    "for index, (obj, fig) in enumerate(zip(objects, figures)):\n",
    "    plt.subplot(2, 2, index+1)\n",
    "    plt.ylim(0,100)\n",
    "    plt.bar(obj,fig, label=\"sdddddddd\", color='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Results of MITIEWNUT17 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### ENTITY F1-SCORES ###\n",
      "processed 23394 tokens with 1079 phrases \n",
      "found: 617 phrases correct: 355.\n",
      "\n",
      "accuracy:  94.18 \n",
      "precision:  57.54 \n",
      "recall:  32.90 \n",
      "FB1:  41.86\n",
      "\n",
      "['corporation', 'person', 'location']\n",
      "      corporation: \n",
      "precision:  31.91 \n",
      "recall:  22.73 \n",
      "FB1:  26.55  15\n",
      "\n",
      "         location: \n",
      "precision:  56.92 \n",
      "recall:  49.33 \n",
      "FB1:  52.86  74\n",
      "\n",
      "           person: \n",
      "precision:  70.72 \n",
      "recall:  50.12 \n",
      "FB1:  58.66  215\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "objects,figures,labels = wnut_eval(test_conll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nudCbmWXOhd7MLHMu9GZmmXOhNzPL3P8B4DMlNdRlpMwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f345966fcd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2)\n",
    "for index, (obj, fig) in enumerate(zip(objects, figures)):\n",
    "    plt.subplot(2, 2, index+1)\n",
    "    plt.ylim(0,100)\n",
    "    plt.bar(obj,fig, label=\"sdddddddd\", color='g')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
